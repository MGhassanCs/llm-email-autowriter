{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Email Autowriter Prototype\n",
        "\n",
        "This notebook contains the original prototype code extracted from the working implementation.\n",
        "The modular application in the `app/` directory is based on this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "import torch\n",
        "\n",
        "# Model loading\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int8\"\n",
        "llm = LLM(model=model_name, dtype=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "class ModelCall():\n",
        "    def __init__(self, temperature:float=0.65, top_p:float=0.95, max_tokens:int=1024):\n",
        "        self.temperature=temperature\n",
        "        self.top_p=top_p\n",
        "        self.max_tokens=max_tokens\n",
        "\n",
        "    def makeQuery(self, query):\n",
        "        from vllm import LLM, SamplingParams\n",
        "        self.sampling_params = SamplingParams(temperature=self.temperature, top_p=self.top_p, max_tokens=self.max_tokens)\n",
        "        if not any(word in query.lower() for word in [\"email\", \"mail\", \"send an email\", \"write an email\", \"compose email\"]):\n",
        "            self.messages = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a strict assistant that only helps with writing emails. \"\n",
        "                        \"If a user gives a prompt that is not related to writing an email, \"\n",
        "                        \"you will ask them to provide a clear instruction to write an email.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": query\n",
        "                }\n",
        "            ]\n",
        "        else:\n",
        "            self.messages = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a helpful email writing assistant. \"\n",
        "                        \"You are going to write an email based on the user's prompt. \"\n",
        "                        \"You will write the email in a professional tone.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": query\n",
        "                }\n",
        "            ]\n",
        "    def retQuery(self):\n",
        "        outputs = llm.chat(self.messages, self.sampling_params)\n",
        "        return outputs[0].outputs[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Usage example\n",
        "llmQNA = ModelCall()\n",
        "llmQNA.makeQuery(\"Write a professional email to my professor requesting a deadline extension for my assignment. Be polite, concise, and include that I've had unexpected personal issues.\")\n",
        "print(llmQNA.retQuery())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
